= How to use the Gruntwork Service Catalog
:type: guide
:description: Learn about production-grade infrastructure, Terraform, Terragrunt, Packer, Docker, immutable infrastructure, versioning for infrastructure code, automated tests for infrastructure code, and more.
// TODO: the image should be a screenshot of the service catalog?
:image: ../assets/img/guides/service-catalog/grunty-blocks.png
:tags: aws, gcp, terraform, terragrunt
:toc:
:toc-placement!:

// GitHub specific settings. See https://gist.github.com/dcode/0cfbf2699a1fe9b46ff04c41721dda74 for details.
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

toc::[]

== Intro

This guide will walk you through how to use the
https://gruntwork.io/infrastructure-as-code-library/[Gruntwork Service Catalog] to build a production-grade tech stack
for your company.

=== What is the Gruntwork Service Catalog and Reference Architecture?

The https://gruntwork.io/infrastructure-as-code-library/[Gruntwork Service Catalog] is a collection of over 300,000
lines of reusable, battle-tested, production-ready infrastructure code for AWS and GCP. Here are the benefits of using
the Gruntwork Service Catalog:

// TODO: add a screenshot of the service catalog

Go to prod in days, not months::
  Most teams have the same basic infrastructure needs (e.g., Docker cluster, load balancer, database, cache, CI/CD,
  monitoring, secrets management, and so on), so instead of wasting months reinventing the wheel and building these
  same pieces from scratch, you get to leverage a library of reusable, battle-tested, off-the-shelf infrastructure
  that has been built by a team of DevOps experts and proven in production at hundreds of companies.

Customize everything using infrastructure as code::
  Everything in the Gruntwork Service Catalog is defined as code (primarily in Terraform, Go, Python, and Bash) and
  you get access to 100% of this code. You can combine and compose this code in any way you wish, see how everything
  works under the hood, debug any issues you run into, and customize and modify the code to fit your exact needs.

Learn best practices::
  The Service Catalog includes both thorough documentation and access to the
  https://gruntwork.io/training/[DevOps Training Library], a collection of video training courses that teach a variety
  of DevOps topics, such as infrastructure as code, Terraform, Docker, Packer, AWS, GCP, security, and more.

Keep everything up to date::
  All of the code in the Gruntwork Service Catalog is semantically versioned and we are constantly releasing new
  versions with the latest best practices, new features, and bug fixes. Never miss a critical security fix again. Never
  waste months updating to the latest version of Terraform. Just follow the instructions in the monthly
  https://blog.gruntwork.io/tagged/gruntwork-newsletter[Gruntwork Newsletter] to get better infrastructure through a
  version number bump.

Get commercial support::
  Get access to a team of DevOps experts who can help you set up your infrastructure, design highly available and
  scalable systems, automate your builds and deployments, troubleshoot issues, and avoid gotchas and pitfalls. Reach out
  to us via Slack, email, and phone/video calls, get code reviews, prioritized bug fixes, and SLAs on response times.

=== What you'll learn in this guide

This guide consists of four main sections:

<<core_concepts>>::
  An overview of the core concepts you need to understand to use the Gruntwork Service Catalog, including a look into
  how the Service Catalog is designed, how the Reference Architecture is designed, how we build production-grade
  infrastructure, and how to make use of infrastructure as code, Terraform, Terragrunt, Packer, Docker, immutable
  infrastructure, versioning, automated testing, and more.

// TODO: link to checklist below
<<production_grade_design>>::
  An overview of how to use the Gruntwork Service Catalog to build a production-grade tech stack. To get a sense of
  what production-grade means, check out <<production_grade_infra_checklist>>.

<<deployment_walkthrough>>::
  A step-by-step guide to deploying several services from the Gruntwork Service Catalog to give you some hands-on
  practice.

<<next_steps>>::
  What to do next once you've finished reading this guide.

Feel free to read the guide start to finish or skip around to whatever part interests you!

[[core_concepts]]
== Core concepts

=== The Gruntwork Service Catalog

// TODO: add a screenshot of the service catalog

The Gruntwork Service Catalog is a collection of over 300,000 lines of reusable, battle-tested infrastructure code.
that is organized into 40+ GitHub repos, some public and open source, and some private and only accessible to Gruntwork
customers. Each repo is focused on one type of infrastructure: e.g., there is one repo that contains code for deploying
and managing Kubernetes on AWS, one repo with code for deploying and managing the ELK stack, one repo that
contains a collection of CI / CD code, and so on.

The code in the Gruntwork Service Catalog is written using a combination of:

Terraform::
  Used to to define and manage most of the basic infrastructure, such as servers, databases, load balancers, and
  networking. More on <<terraform>> later.

Bash::
  Used for _install scripts_ (e.g., a Bash script that you can use to create Linux VM image with Elasticsearch
  installed) and _run scripts_ (e.g., you use Terraform to deploy a cluster of server with the Elasticsearch VM image
  on it and configure the servers to execute a Bash script during boot that can auto-discover and bootstrap the
  Elasticsearch cluster).

Python::
  Used for more complicated scripts, especially those that need to run on other operating systems (e.g., Windows)
  and/or those that need to be called directly from Terraform (e.g., to fill in some missing functionality).

Go::
  Used to build complex, cross-platform CLI applications. E.g., We have an app called `ssh-grunt` you can run on each
  server to manage SSH access to that server via IAM groups.

Why these languages? We wrote an entire blog post on
https://blog.gruntwork.io/why-we-use-terraform-and-not-chef-puppet-ansible-saltstack-or-cloudformation-7989dad2865c[why we use Terraform];
as for Bash, Python, and Go, we use them because they work just about everywhere, with few or no external dependencies,
and they can be integrated with almost any configuration management tool: e.g., you can use Bash scripts with Chef,
Puppet, Ansible, Packer, and Docker (more on <<packer>> and <<docker>> later).

The code in each repo is organized into three primary folders, `modules`, `examples`, and `test`, as described in the
following sections.

=== Modules

// TODO: add a screenshot of the modules folder

Each repo in the Gruntwork Service Catalog contains a `modules` folder that contains the main implementation code,
broken down into standalone, orthogonal, reusable, highly configurable _modules_. For example, the ELK repo
(Elasticsearch, Logstash, Kubernetes) isn't one giant module that deploys the entire ELK stack, but a bunch of separate
modules for installing, running, and deploying Elasticsearch, Kibana, Logstash, Elastalert, Beats, Collectd, and so on.

This allows you to combine and compose the modules in many different permutations to fit your exact needs: e.g., some
use cases need the full ELK stack, while some solely use Elasticsearch; sometimes you run Elasticsearch, Logstash, and
Kibana each in completely separate clusters (e.g., in prod, for high availability and scalability) and sometimes you
run them all in a single cluster or single node (e.g., in dev, to save money).

=== Examples

// TODO: add a screenshot of some example code

Each repo in the Gruntwork Service Catalog contains an `examples` folder that shows you how to assemble the modules
from the `modules` folder into different permutations. This lets you try the modules out in minutes, without having to
write a line of code. In other words, this is executable documentation.

=== Automated tests

// TODO: add a screenshot of some test code

Each repo in the Gruntwork Service Catalog contains a `test` folder that contains automated tests for the examples in
the `examples` folder. These are mostly integration tests, which use
https://github.com/gruntwork-io/terratest/[Terratest] under the hood to deploy the examples into real environments
(e.g., real AWS and GCP accounts), validate that everything works, and then tear everything down.

For example, after every commit to the ELK repo, we spin up a dozen ELK clusters, perform a variety of validation steps
(e.g., read data, write data, access Kibana, etc.) and then tear it all down again. This is how we build confidence
that the code does what we say it does—and that it continues to do it over years of updates.

=== Versioning

All of the code in the Gruntwork Service Catalog is _versioned_. Every time we make a change, we put out a new
versioned release, and announce it in the monthly
https://blog.gruntwork.io/tagged/gruntwork-newsletter[Gruntwork Newsletter].

When you use the code from the Gruntwork Service Catalog (a topic we'll cover in <<deployment_walkthrough>>), you will
always pin yourself to a specific version of the code. That way, you are not accidentally affected by any subsequent
changes in the Gruntwork Service Catalog until you explicitly choose to pull those changes in. And when you do want to
pull the changes in, it's just a matter of bumping the version number!

We use version numbers of the form `MAJOR.MINOR.PATCH` (e.g., `1.2.3`), following the principles of
_https://semver.org[semantic versioning]_. In traditional semantic versioning, you increment the:

. MAJOR version when you make incompatible API changes,
. MINOR version when you add functionality in a backwards compatible manner, and
. PATCH version when you make backwards compatible bug fixes.

However, much of the Gruntwork Service Catalog is built on Terraform, and as Terraform is still not at version `1.0.0`
(latest version as of August, 2019, was `0.12.6`), most of the Gruntwork Service Catalog is using `0.MINOR.PATCH`
version numbers. With `0.MINOR.PATCH`, the rules are a bit different, where you increment the:

. MINOR version when you make incompatible API changes
. PATCH version when you add backwards compatible functionality or bug fixes.

=== The Gruntwork Reference Architecture

// TODO: Ref Arch diagram

The Gruntwork Reference Architecture is a production-grade, end-to-end tech stack built on top of the modules from the
Gruntwork Service Catalog. It includes just about everything the typical company needs: multiple environments, each
configured with server orchestration (e.g., Kubernetes), load balancers, databases, caches, network topology,
monitoring, alerting, log aggregation, CI/CD, user management, secrets management, SSH management, VPN management, and
so on. We wire all these pieces together according to your needs, deploy it into your AWS or GCP accounts, and give you
100% of the code—all in about one day.

Whereas the Gruntwork Service Catalog is relatively unopinionated, allowing you to combine and compose modules, tools,
and approaches however you want ("a la carte"), the Gruntwork Reference Architecture is more opinionated, giving you a
small set of pre-defined, standardized sets of modules, tools, and approaches to choose from ("prixe fixe"). For a
detailed look at the Gruntwork Reference Architecture, check out
https://blog.gruntwork.io/how-to-build-an-end-to-end-production-grade-architecture-on-aws-part-1-eae8eeb41fec[How to Build an End to End Production-Grade Architecture on AWS].

If the opinionated design of the Reference Architecture looks like a good fit for your company, you may wish to
purchase it as a way to save months on having to wire everything together and deploy it yourself. If the opinionated
design is not a good fit, then you will want to use the Gruntwork Service Catalog directly instead.

[[production_grade_infra_checklist]]
=== The production-grade infrastructure checklist

The Gruntwork Service Catalog is a collection of _production-grade infrastructure_—that is, the type of reliable,
secure, battle-tested infrastructure that you'd bet your company on. Every time you deploy infrastructure, you're
betting that your infrastructure won’t fall over if traffic goes up; you're betting that your infrastructure won't lose
your data if there's an outage; you're betting that your infrastructure won't allow your data to be compromised when
hackers try to break in; and if these bets don't work out, your company may go out of business. That's what's at stake
when we say "production-grade."

Building production-grade infrastructure requires taking into account a long list of details, which we have captured in
_The Production-Grade Infrastructure Checklist_:

. The Production-Grade Infrastructure Checklist
|===
| Task | Description | Example tools

| Install
| Install the software binaries and all dependencies.
| Bash, Chef, Ansible, Puppet

| Configure
| Configure the software at runtime. Includes port settings, TLS certs, service discovery, leaders, followers, replication, etc.
| Bash, Chef, Ansible, Puppet

| Provision
|  Provision the infrastructure. Includes EC2 instances, load balancers, network topology, security gr oups, IAM permissions, etc.
| Terraform, CloudFormation

| Deploy
| Deploy the service on top of the infrastructure. Roll out updates with no downtime. Includes blue-green, rolling, and canary deployments.
| Scripts, Orchestration tools (ECS, k8s, Nomad)

| High availability
| Withstand outages of individual processes, EC2 instances, services, Availability Zones, and regions.
| Multi AZ, multi-region, replication, ASGs, ELBs

| Scalability
| Scale up and down in response to load. Scale horizontally (more servers) and/or vertically (bigger servers).
| ASGs, replication, sharding, caching, divide and conquer

| Performance
| Optimize CPU, memory, disk, network, GPU, and usage. Includes query tuning, benchmarking, load testing, and profiling.
| Dynatrace, valgrind, VisualVM, ab, Jmeter

| Networking
| Configure static and dynamic IPs, ports, service discovery, firewalls, DNS, SSH access, and VPN access.
| EIPs, ENIs, VPCs, NACLs, SGs, Route 53, OpenVPN

| Security
| Encryption in transit (TLS) and on disk, authentication, authorization, secrets management, server hardening.
| ACM, EBS Volumes, Cognito, Vault, CIS

| Metrics
| Availability metrics, business metrics, app metrics, server metrics, events, observability, tracing, and alerting.
| CloudWatch, DataDog, New Relic, Honeycomb

| Logs
| Rotate logs on disk. Aggregate log data to a central location.
| CloudWatch logs, ELK, Sumo Logic, Papertrail

| Backup and Restore
| Make backups of DBs, caches, and other data on a scheduled basis. Replicate to separate region/account.
| RDS, ElastiCache, ec2-snapper, Lambda

| Cost optimization
| Pick proper instance types, use spot and reserved instances, use auto scaling, and nuke unused resources.
| ASGs, spot instances, reserved instances

| Documentation
| Document your code, architecture, and practices. Create playbooks to respond to incidents.
| READMEs, wikis, Slack

| Tests
| Write automated tests for your infrastructure code. Run tests after every commit and nightly.
| Terratest
|===

Most other collections of infrastructure code and service catalogs (e.g., AWS Quick Starts, Bitnami Application Catalog,
the Terraform Registry, Ansible Galaxy, Chef Supermarket, etc) are useful for learning and example code, but they do
not take most of this checklist into account, and therefore are not a good fit for direct production use. On the other
hand, every module in the Gruntwork Service Catalog goes through the production-grade checklist and is explicitly
designed for use directly in production.

=== Infrastructure as code

Everything in the Gruntwork Service Catalog is designed to allow you to define your _infrastructure as code (IaC)_.
That is, instead of deploying infrastructure _manually_ (e.g., by clicking around a web page), the idea behind IaC is
to write code to define, provision, and manage your infrastructure. This has a number of benefits:

Self-service::
  Most teams that deploy code manually have a small number of sysadmins (often, just one) who are the only ones who
  know all the magic incantations to make the deployment work and are the only ones with access to production. This
  becomes a major bottleneck as the company grows. If your infrastructure is defined in code, then the entire
  deployment process can be automated, and developers can kick off their own deployments whenever necessary.

Speed and safety::
  If the deployment process is automated, it'll be significantly faster, since a computer can carry out the deployment
  steps far faster than a person; and safer, since an automated process will be more consistent, more repeatable, and
  not prone to manual error.

Documentation::
  Instead of the state of your infrastructure being locked away in a single sysadmin's head, you can represent the
  state of your infrastructure in source files that anyone can read. In other words, IaC acts as documentation,
  allowing everyone in the organization to understand how things work, even if the sysadmin goes on vacation.

Version control::
  You can store your IaC source files in version control, which means the entire history of your infrastructure is now
  captured in the commit log. This becomes a powerful tool for debugging issues, as any time a problem pops up, your
  first step will be to check the commit log and find out what changed in your infrastructure, and your second step may
  be to resolve the problem by simply reverting back to a previous, known-good version of your IaC code.

Validation::
  If the state of your infrastructure is defined in code, then for every single change, you can perform a code review,
  run a suite of automated tests, and pass the code through static analysis tools, all practices that are known to
  significantly reduce the chance of defects.

Happiness::
  Deploying code and managing infrastructure manually is repetitive and tedious. Developers and sysadmins resent this
  type of work, as it involves no creativity, no challenge, and no recognition. You could deploy code perfectly for
  months, and no one will take notice—until that one day when you mess it up. That creates a stressful and unpleasant
  environment. IaC offers a better alternative that allows computers to do what they do best (automation) and
  developers to do what they do best (coding).

Reuse::
  You can package your infrastructure into reusable modules, so that instead of doing every deployment for every
  product in every environment from scratch, you can build on top of known, documented, battle-tested pieces. You
  can build these reusable modules yourself or use an existing collection of modules, such as the Gruntwork Service
  Catalog.

Some of the main IaC tools you'll see used and referenced in the Gruntwork Service Catalog are Terraform, Terragrunt,
Packer, and Docker, each of which we'll discuss in the next several sections.

[[terraform]]
=== Terraform

https://www.terraform.io[Terraform] is an open source _provisioning_ tool that allows you to define and manage as code a
wide variety of infrastructure (e.g., servers, load balancers, databases, network settings, and so on) across
a wide variety of _providers_ (e.g., AWS, GCP, Azure). For example, here's some example Terraform code you can use to
deploy an EC2 instance (a virtual server) running Ubuntu 18.04 into the `us-east-2` region of AWS:

.terraform-example.tf
[source,hcl]
----
# Deploy to the us-east-2 region of AWS
provider "aws" {
  region = "us-east-2"
}

# Deploy an EC2 instance running Ubuntu 18.04
resource "aws_instance" "example" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"
}
----

You can deploy this server by running `terraform init` and `terraform apply`. Check out the
https://blog.gruntwork.io/a-comprehensive-guide-to-terraform-b3d32832baca[Comprehensive Guide to Terraform] for a
thorough introduction to the language.

A large percentage of the infrastructure code in the Gruntwork Service Catalog is defined using Terraform. We even
wrote https://www.terraformupandrunning.com[the book] on it!

=== Terragrunt

https://github.com/gruntwork-io/terragrunt[Terragrunt] is a thin, open source wrapper for Terraform. It is designed to
fill in some missing features in Terraform, such as allowing you to define your Terraform backend configuration in
one `terragrunt.hcl` file, rather than having to copy/paste the same config over and over again:

.terragrunt.hcl
[source,hcl]
----
remote_state {
  backend = "s3"
  config = {
    bucket         = "my-terraform-state"
    key            = "${path_relative_to_include()}/terraform.tfstate"
    region         = "us-east-1"
    encrypt        = true
    dynamodb_table = "my-lock-table"
  }
}
----

Check out
https://blog.gruntwork.io/terragrunt-how-to-keep-your-terraform-code-dry-and-maintainable-f61ae06959d8[Terragrunt: how to keep your Terraform code DRY and maintainable]
for a thorough introduction.

Note that while the Gruntwork Reference Architecture relies on Terragrunt as one of its opinionated tools, the
Gruntwork Service Catalog does NOT require Terragrunt; you can use the Terraform modules in the Gruntwork
Service Catalog with vanilla Terraform, Terraform Enterprise, Atlantis, Terragrunt, or any other tools you prefer.

[[packer]]
=== Packer

https://www.packer.io[https://www.packer.io] is an open source tool you can use to define _machine images_ (e.g., VM
images, Docker images) as code. For example, here is how you can use Packer to define an Ubuntu 18.04 Amazon Machine
Image (AMI) that has Node.js installed:

.packer-example.json
[source,json]
----
{
  "builders": [{
    "type": "amazon-ebs",
    "region": "us-east-2",
    "source_ami": "ami-0c55b159cbfafe1f0",
    "instance_type": "t2.micro",
    "ssh_username": "ubuntu",
    "ami_name": "packer-example-{{timestamp}}"
  }],
  "provisioners": [{
    "type": "shell",
    "inline": [
      "curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -",
      "sudo apt-get update -y",
      "sudo apt-get install -y nodejs"
    ]
  }]
}
----

You can run `packer build packer-example.json` to build an AMI from this code and then deploy this AMI to your AWS
account using other tools. For example, the Gruntwork Service Catalog contains several Terraform modules that can
deploy AMIs across one or more servers (e.g., into an AWS Auto Scaling Group), with support for auto scaling, auto
healing, zero-downtime deployments, etc.

The Gruntwork Service Catalog contains a number of scripts and binaries that you can run on your servers: e.g., the
ELK code includes scripts you run during boot on Elasticsearch servers to bootstrap the cluster; you can also run
the `ssh-grunt` binary on each server to allow manage SSH access to that server using IAM groups (i.e., IAM users in
specific IAM groups will be able to SSH to specific servers using their own usernames and SSH keys).

To get these scripts and binaries onto your virtual servers (e.g., onto EC2 instances in AWS or compute instances in
GCP), we recommend using Packer to build VM images that have these scripts and binaries installed. You'll see an
example of how to do this in the <<deployment_walkthrough>>. Note that Gruntwork Service Catalog does NOT require that
you use Packer (e.g., you could also use Ansible or Chef to install the scripts and binaries), but the Gruntwork
Reference Architecture does use Packer as one of its opinionated tools.

[[docker]]
=== Docker

https://www.docker.com[Docker] is an open source tool you can use to run _containers_ and define _container images_ as
code. A container is a bit like a lightweight VM, except instead of virtualizing all the hardware and OS, containers
virtualize solely user space, which gives you many of the isolation benefits of a VM (e.g., each container is isolated
in terms of memory, CPU, networking, hard drive, etc), but with much less memory, CPU, and start-up time overhead.
For example, here is how you can define an Ubuntu 18.04 Docker image that has Node.js installed:

.packer-example.json
[source,Dockerfile]
----
FROM ubuntu:18.04

RUN curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash - && \
    sudo apt-get update -y && \
    sudo apt-get install -y nodejs
----

You can run `docker build -t example-image .` to build a Docker image from this code and then deploy the Docker image
using a _container orchestration tool_ such as Kubernetes, ECS, nor Nomad (all of these tools are available in the
Gruntwork Service Catalog).

[[immutable_infrastructure]]
=== Immutable infrastructure

With _mutable infrastructure_, you deploy a set of servers, and you continuously update those servers in place. Every
new update gets installed on top of the previous updates, either manually (e.g., by SSHing to each server and running
commands), or via tools like Ansible, Chef, or Puppet. Over time, each "mutable" server builds up a history of
changes, which can make it difficult to (a) reason about what's actually installed and (b) debug issues that are
specific to the unique history of one server but not others.

The idea behind _immutable infrastructure_ is that once you deploy a server, you never change it again. If you need to
roll out an update, you deploy a _new_ server with that update, and undeploy the old one. This paradigm is built for use
with (a) the cloud, where you can easily spin up or tear down servers on-demand and (b) machine images, as every time
there's a change, you can use tools like Packer or Docker to build a new, immutable, versioned machine image (e.g., VM
image or Docker image), and deploy new servers with that image.

The advantages of immutable infrastructure are:

Easier to reason about servers::
  It's much easier to figure out what's installed on any server, as you know the exact image each server is running,
  and that the image never changes.

You can run the same images in all environments::
  For example, you can run the same Docker image on your laptop and in production (whereas it's rare to run Ansbile,
  Chef, or Puppet in local dev). This helps to reduce "works on my machine" and environment-specific bugs, and makes it
  easier to debug those issues when they do happen.

Easier scaling and rollback::
  With immutable images, you can quickly and easily spin up 100 or 1,000 servers, with no need to worry about how long
  it'll take to configure all those servers (e.g., via Ansible, Chef, or Puppet), as all the configuration has already
  happened and is captured in the VM or Docker image. Rollback is easier too, as you can quickly jump back to a
  previous image, without having to wait for and worry about running a bunch of older install commands (which may no
  longer work, e.g., if certain packages have been removed from APT or YUM).


[[production_grade_design]]
== Production-grade design

With all the core concepts out of the way, let's now discuss how to use the Gruntwork Service Catalog to build
production-grade infrastructure.

- infra-modules
- infra-live
- Packer
- Docker


[[deployment_walkthrough]]
== Deployment walkthrough

Let's now walk through how to deploy a production-grade VPC using the Gruntwork Service Catalog.

[[pre_requisites]]
=== Pre-requisites

This walkthrough has the following pre-requistes:

=== Outline

- Give examples using open source modules?
- How to consume the code (Terraform, scripts, binaries)
- How to use with TFE?

[[next_steps]]
== Next steps

TODO
