---
title: ASG Service Migration Guide
categories: Upgrades
image: /assets/img/guides/refresh_icon.png
excerpt: Learn how to update your v1 Reference Architecture to use the Gruntwork Service Catalog.
tags: ["aws", "terraform", "terragrunt"]
cloud: ["aws"]
redirect_from: /static/guides/upgrades/how-to-update-your-ref-arch/
---
:page-type: guide
:page-layout: post

:toc:
:toc-placement!:

// GitHub specific settings. See https://gist.github.com/dcode/0cfbf2699a1fe9b46ff04c41721dda74 for details.
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
toc::[]
endif::[]

== ASG Service Migration Guide

Follow this guide to update the ASG service to the Service Catalog.

Cross-reference with the example changes shown in these patches:
* link:https://github.com/gruntwork-io/infrastructure-live-multi-account-acme/blob/master/dev/us-east-1/dev/services/sample-app-backend-multi-account-acme-asg/ref-arch-v1-to-service-catalog-migration.patch[asg (backend)]
* link:https://github.com/gruntwork-io/infrastructure-live-multi-account-acme/blob/master/dev/us-east-1/dev/services/sample-app-frontend-multi-account-acme-asg/ref-arch-v1-to-service-catalog-migration.patch[asg (frontend)]

=== Estimated Time to Migrate: 1 hour per environment

=== AMI

Update the packer template for your service AMI to install the
https://github.com/gruntwork-io/terraform-aws-service-catalog/tree/master/modules/base/ec2-baseline[ec2-baseline]
service module. The `ec2-baseline` service module installs a set of recommended core components for EC2 instances,
including https://github.com/gruntwork-io/terraform-aws-security/tree/master/modules/ssh-grunt[ssh-grunt],
https://github.com/gruntwork-io/terraform-aws-security/tree/master/modules/ip-lockdown[ip-lockdown],
https://github.com/gruntwork-io/terraform-aws-monitoring/tree/master/modules/logs/cloudwatch-log-aggregation-scripts[cloudwatch
logs agent], https://github.com/gruntwork-io/terraform-aws-security/tree/master/modules/fail2ban[fail2ban], and others.

*You must build a new AMI using `ec2-baseline` to update your ASG service to the Service Catalog module*. The Service
Catalog `asg-service` module provides a base `user-data.sh` script that invokes the scripts in the `ec2-baseline`. Your
service will fail to boot up if you are missing the `ec2-baseline` utility scripts.

To update your `packer` template, it is recommended to start with
https://github.com/gruntwork-io/terraform-aws-service-catalog/blob/master/examples/for-learning-and-testing/services/asg-service/ami-example.json[the
`asg-service` example `packer` template] and extend it with the logic you need for your application.

Refer to the link:sample-app-backend-packer-template-diff.patch[following diff] for an example of updating the Reference
Architecture sample app packer template.

Once you build the new AMI, update the `ami` input variable with the new AMI ID.

=== cloud_init_parts (user data script)

Configure the `cloud_init_parts` input variable to provide the cloud init scripts (boot script) for your application.
Cloud-init is a startup configuration utility for cloud compute instances that AWS EC2 instances support.

To configure the boot script, you can either directly provide the script contents in the `terragrunt.hcl` file, or
templatize the script and use the https://www.terraform.io/docs/language/functions/templatefile.html[templatefile] hcl
function to render it.

For example, if you had the following user data and template data source in your module:

_user-data.sh_

[source,bash]
----
#!/bin/bash

set -e

# Send the log output from this script to user-data.log, syslog, and the console
# From: https://alestic.com/2010/12/ec2-user-data-output/
exec > >(tee /var/log/user-data.log|logger -t user-data -s 2>/dev/console) 2>&1

# The variables in the code below should be filled in by Terraform interpolation
"${init_script_path}" \
  --aws-region "${aws_region}" \
  --vpc-name "${vpc_name}" \
  --asg-name "${asg_name}" \
  --port "${server_port}" \
  --db-url "${db_url}" \
  --memcached-url "${memcached_url}" \
  --internal-alb-url "${internal_alb_url}" \
  --internal-alb-port "${internal_alb_port}"
----

_main.tf user data template data source_

[source,hcl]
----
data "template_file" "user_data" {
  template = file("${path.module}/user-data/user-data.sh")

  vars = {
    init_script_path  = var.init_script_path
    aws_region        = var.aws_region
    asg_name          = var.name
    vpc_name          = data.terraform_remote_state.vpc.outputs.vpc_name
    server_port       = var.server_port
    db_url            = data.terraform_remote_state.db.outputs.primary_endpoint
    memcached_url     = join(",", data.terraform_remote_state.memcached.outputs.cache_addresses)
    internal_alb_url  = data.terraform_remote_state.alb_internal.outputs.alb_dns_name
    internal_alb_port = 80
  }
}
----

You can convert it to the terragrunt inputs in the following ways:

[arabic]
. Directly specifying user data:
+
_terragrunt.hcl_
+
....
dependency "vpc" {
  config_path = "../../vpc"
}
dependency "postgres" {
  config_path = "../../data-stores/postgres"
}
dependency "memcached" {
  config_path = "../../data-stores/memcached"
}
dependency "alb_internal" {
  config_path = "../../networking/alb-internal"
}

locals {
  region      = "us-east-2"
  full_name   = "asg-sample-app-dev"
  server_port = 8080
}

inputs = {
  # Specify the application boot script
  cloud_init_parts = {
    app_boot_script = {
      filename     = "app_boot_script.sh"
      content_type = "text/x-shellscript"
      content      = <<-EOF
      #!/bin/bash

      set -e

      # Send the log output from this script to user-data.log, syslog, and the console
      # From: https://alestic.com/2010/12/ec2-user-data-output/
      exec > >(tee /var/log/user-data.log|logger -t user-data -s 2>/dev/console) 2>&1

      /opt/refarch-demo-sample-app-backend/bin/run-app.sh \
        --aws-region '${local.region}' \
        --vpc-name '${dependency.vpc.outputs.vpc_name}' \
        --asg-name '${local.full_name}' \
        --port '${local.server_port}' \
        --db-url '${dependency.postgres.outputs.primary_endpoint}' \
        --memcached-url '${join(",", dependency.memcached.outputs.cache_addresses)}' \
        --internal-alb-url '${dependency.alb_internal.outputs.alb_dns_name}' \
        --internal-alb-port '80'
      EOF
    }
  }
}
....
. Using a shared template file
+
_user-data.sh.tpl_
+
[source,bash]
----
#!/bin/bash

set -e

# Send the log output from this script to user-data.log, syslog, and the console
# From: https://alestic.com/2010/12/ec2-user-data-output/
exec > >(tee /var/log/user-data.log|logger -t user-data -s 2>/dev/console) 2>&1

# The variables in the code below should be filled in by Terraform interpolation
"${init_script_path}" \
  --aws-region "${aws_region}" \
  --vpc-name "${vpc_name}" \
  --asg-name "${asg_name}" \
  --port "${server_port}" \
  --db-url "${db_url}" \
  --memcached-url "${memcached_url}" \
  --internal-alb-url "${internal_alb_url}" \
  --internal-alb-port '80'
----
+
_terragrunt.hcl_
+
[source,hcl]
----
dependency "vpc" {
  config_path = "../../vpc"
}
dependency "postgres" {
  config_path = "../../data-stores/postgres"
}
dependency "memcached" {
  config_path = "../../data-stores/memcached"
}
dependency "alb_internal" {
  config_path = "../../networking/alb-internal"
}

locals {
  region      = "us-east-2"
  full_name   = "asg-sample-app-dev"
  server_port = 8080
}

inputs = {
  # Specify the application boot script
  cloud_init_parts = {
    app_boot_script = {
      filename     = "app_boot_script.sh"
      content_type = "text/x-shellscript"
      content = templatefile(
         "${get_parent_terragrunt_dir()}/templates/user-data.sh.tpl",
         {
           aws_region       = local.region
           vpc_name         = dependency.vpc.outputs.vpc_name
           asg_name         = local.full_name
           server_port      = local.server_port
           db_url           = dependency.postgres.outputs.primary_endpoint
           memcached_url    = join(",", dependency.memcached.outputs.cache_addresses)
           internal_alb_url = dependency.alb_internal.outputs.alb_dns_name
         },
      )
    }
  }
}
----

=== forward_listener_rules (ALB listener config)

Update your configuration of listener rules. Before, the listener rules were all configured internally in the module
using the `is_internal_alb` and `alb_listener_rule_configs` input variables. Now you need to configure them using the
`listener_arns`, `listener_ports`, and `forward_listener_rules` input variables (using dependencies to look up which ALB
to bind the rules to).

For example, if you had the following config:

[source,hcl]
----
# BEFORE
inputs = {
  is_internal_alb = true
  alb_listener_rule_configs = [{
    path     = "/refarch-demo-sample-app-backend*"
    priority = 100
  }]
}
----

Change the config to:

[source,hcl]
----
dependency "alb_internal" {
  config_path = "../../networking/alb-internal"
}

# AFTER
inputs = {
  default_listener_arns  = dependency.internal_alb.outputs.listener_arns
  default_listener_ports = ["443"] # NOTE: this should be the same as the main port for the ALB
  forward_rules = {
    main = {
      path     = "/refarch-demo-sample-app-backend*"
      priority = 100
    }
  }
}
----

=== server_ports (Server listener config)

Update your configuration of server listeners and corresponding ALB health checks. Before, these were managed using the
`server_port`, `health_check_path`, and `health_check_protocol` input variables. Now you need to configure them using
the `server_ports` input map.

For example, if you had the following config:

[source,hcl]
----
# BEFORE
inputs = {
  server_port           = local.server_port
  health_check_path     = "/refarch-demo-sample-app-backend/health"
  health_check_protocol = "HTTP"
}
----

Change the config to:

[source,hcl]
----
# AFTER
inputs = {
  server_ports = {
    http = {
      server_port            = local.server_port
      health_check_path      = "/refarch-demo-sample-app-backend/health"
      protocol               = "HTTP"

      # Backward compatibility: Set the target group name directly so that we avoid recreating it.
      target_group_name = local.full_name
    }
  }
}
----

=== New Required Inputs

Configure these new inputs to migrate to the Service Catalog version of the module. They are now required.

* `vpc_id`: Set this to the ID of the VPC where the service ASG should be deployed. This should be pulled in using a
`dependency` block against the `vpc-app` service, using the `vpc_id` output.
* `subnet_ids`: Set this to the list of IDs of the VPC subnet where the service ASG should be deployed. This should be
pulled in using a `dependency` block against the `vpc-app` service, using the `private_app_subnet_ids` output.
* `ami_filters`: Set this to `null` . This provides an alternative mechanism to lookup the AMI to use dynamically, but
since you are providing the AMI ID directly, this variable needs to be turned off.

=== Inputs for Backward Compatibility

Configure the following new inputs to ensure your service continues to function with minimal interruption. These are
necessary to maintain backward compatibility. _If left unset, you will risk redeploying the service and causing
downtime._

* `alarms_sns_topic_arn`: The ARNs of SNS topics for receiving alerts from CloudWatch. This should be pulled in with a
`dependency` block against the `sns-topic` service, using the `topic_arn` output.
* `alarm_sns_topic_arns_us_east_1`: The ARNs of SNS topics for receiving alerts from CloudWatch in `us-east-1` (route 53
health check alerts only report in the `us-east-1` region). This should be pulled in with a `dependency` block against
the `sns-topic-us-east-1` service, using the `topic_arn` output.
* If you are using `gruntkms` for your secrets management, set the following to ensure the ECS task IAM role retains the
policy to access the KMS key:
+
[source,hcl]
----
iam_policy = {
  KMSKeyAccess = {
    actions   = ["kms:Decrypt"]
    effect    = "Allow"
    resources = [dependency.kms_key.outputs.key_arn]
  }
}
----

=== Renamed Inputs

Rename the following inputs to use the Service Catalog version of the module:

* `keypair_name` â‡’ `key_pair_name`

=== Removed Inputs

Remove the following inputs as they are not present in the Service Catalog version of the module:

* `db_remote_state_path`
* `memcached_remote_state_path`
* `redis_remote_state_path`
* `is_internal_alb`
* `init_script_path`

=== Output Changes

Update downstream dependency references to use the new names of these outputs, which were renamed in the Service Catalog
version of the module.

* `alb_dns_name` has been removed from the module, due to not having access to the output. If you would like to see the
ALB dns name on apply, it is recommended to add a
https://terragrunt.gruntwork.io/docs/features/before-and-after-hooks/[terragrunt after hook] on apply to echo out from
the dependency.
+
[source,hcl]
----
terraform {
  source = "git::ssh://git@github.com/gruntwork-io/terraform-aws-service-catalog.git//modules/services/asg-service?ref=v0.35.1"

  after_hook "show_dns_name" {
    commands = ["apply"]
    execute  = ["echo", "ALB DNS (use if no FQDN for service): ${dependency.alb_internal.outputs.alb_dns_name}"]
  }
}
----

=== State Migration Script

Run the link:./scripts/migrate_asg_service.sh[provided migration script] to migrate the state in a backward compatible way.

*NOTE*: When calling the script, you must provide the name of the server port key (as set in
link:#server_ports-server-listener-config[server_ports]) and the mapping from old listener indexes to listener ports
from the link:#forward_listener_rules-alb-listener-config[listener_rule_configs].

For example, if you had the following before and after configurations:

_BEFORE_

[source,hcl]
----
inputs = {
  alb_listener_rule_configs = [{
    port     = 80
    path     = "/refarch-demo-sample-app-backend*"
    priority = 100
  }, {
    port     = 443
    path     = "/refarch-demo-sample-app-backend*"
    priority = 100
  }]
}
----

_AFTER_

[source,hcl]
----
inputs = {
  server_ports = {
    http = {
      server_port            = local.server_port
      health_check_path      = "/refarch-demo-sample-app-backend/health"
      protocol               = "HTTP"

      # Backward compatibility: Set the target group name directly so that we avoid recreating it.
      target_group_name = local.full_name
    }
  }

  default_listener_ports = ["80", "443"]
  forward_rules = {
    main = {
      path     = "/refarch-demo-sample-app-backend*"
      priority = 100
    }
  }
}
----

Call the migrate script with the following args:

[source,bash]
----
# http comes from server_ports key, while the mapping for 0=80 and 1=443 comes from alb_listener_rule_configs list indices.
./migrate_asg_service.sh http 0=80 1=443
----

=== Breaking Changes

* *Cluster outage*.
** Due to the way the `asg-service` module is designed, any change to the cluster configuration (such as the user data
script) will result in a rebuild of the ASG service. However, the module will do a blue-green deployment. Specifically,
during the `terraform apply` step, the module will create a new ASG with the updated configuration, wait until all the
instances come up, are registered to the ALB, and are reported healthy, before tearing down the old ASG. This means that
you can only expect downtime if there are any issues with getting the new instances up.
** A number of IAM policies were reorganized in the module. This translates to a few recreations of IAM policies
(`destroy` + `create`). Since they apply at the policy level, these should not cause any service disruptions. However,
you may experience a brief (<1 minute) outage in AWS access from your services while the IAM policies are being
recreated.
** The CloudWatch alarms for the ASG were reorganized in the module. This translates to a few recreations of the
CloudWatch Alarms (`destroy` + `create`), and you may experience a brief (<1 minute) outage in monitoring alarms.
